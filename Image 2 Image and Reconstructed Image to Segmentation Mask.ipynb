{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import random\n",
    "import os \n",
    "import numpy as np\n",
    "from scipy.ndimage import sobel\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tensorboardX import SummaryWriter "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Auto Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOENCODER\n",
      "Latent's Shape: torch.Size([1, 512, 4, 4])\n",
      "Output's ShapeL torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, blk, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.blk = blk\n",
    "        self.conv1_a = nn.Conv2d(in_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.conv1_b = nn.Conv2d(3, in_channels, 3, 1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2)) \n",
    "\n",
    "    def forward(self, x, scale_img=\"none\"):\n",
    "        if ((self.blk==\"first\") or (self.blk==\"bottleneck\")):\n",
    "            x1 = self.relu(self.conv1_a(x))\n",
    "            x1 = self.relu(self.conv2(x1))\n",
    "        else:\n",
    "            skip_x = self.relu(self.conv1_b(scale_img))\n",
    "            x1 = torch.cat([skip_x, x], dim=1)\n",
    "            x1 = self.relu(self.conv2(x1))\n",
    "            x1 = self.relu(self.conv3(x1))\n",
    "        out = self.maxpool(self.dropout(x1))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor = 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.upsample(x)\n",
    "        x1 = self.relu(self.conv1(x1))\n",
    "        x1 = self.relu(self.conv2(x1))\n",
    "        x1 = self.relu(self.conv3(x1))\n",
    "        out = self.dropout(x1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepSupervisionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor = 2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, 1, padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.upsample(x)\n",
    "        x1 = self.relu(self.conv1(x1))\n",
    "        x1 = self.relu(self.conv2(x1))\n",
    "        out = self.sigmoid(self.conv3(x1))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        filters = [8, 16, 32, 64, 128, 512] \n",
    "        self.drp_out = 0.3\n",
    "        self.scale_img = nn.AvgPool2d(2, 2)   \n",
    "\n",
    "        self.block_1 = EncoderBlock(\"first\", 3, filters[0])\n",
    "        self.block_2 = EncoderBlock(\"second\", filters[0], filters[1])\n",
    "        self.block_3 = EncoderBlock(\"third\", filters[1], filters[2])\n",
    "        self.block_4 = EncoderBlock(\"fourth\", filters[2], filters[3])\n",
    "        self.block_5 = EncoderBlock(\"fifth\", filters[3], filters[4])\n",
    "        self.block_6 = EncoderBlock(\"bottleneck\", filters[4], filters[5])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-scale input\n",
    "        scale_img_2 = self.scale_img(x)\n",
    "        scale_img_3 = self.scale_img(scale_img_2)\n",
    "        scale_img_4 = self.scale_img(scale_img_3)  \n",
    "        scale_img_5 = self.scale_img(scale_img_4)\n",
    "\n",
    "        x1 = self.block_1(x)\n",
    "        x2 = self.block_2(x1, scale_img_2)\n",
    "        x3 = self.block_3(x2, scale_img_3)\n",
    "        x4 = self.block_4(x3, scale_img_4)\n",
    "        x5 = self.block_5(x4, scale_img_5)\n",
    "        x6 = self.block_6(x5)\n",
    "        return x6\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        filters = [512, 128, 64, 32, 16, 8]\n",
    "        self.drp_out = 0.3\n",
    "\n",
    "        self.block_5 = DecoderBlock(filters[0], filters[1])\n",
    "        self.block_4 = DecoderBlock(filters[1], filters[2])\n",
    "        self.block_3 = DecoderBlock(filters[2], filters[3])\n",
    "        self.block_2 = DecoderBlock(filters[3], filters[4])\n",
    "        self.block_1 = DecoderBlock(filters[4], filters[5])\n",
    "        self.ds = DeepSupervisionBlock(filters[5], 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block_5(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_1(x)\n",
    "        out9 = self.ds(x)\n",
    "        return out9\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        output = self.decoder(latent)\n",
    "        return latent, output\n",
    "\n",
    "\n",
    "\n",
    "print(\"AUTOENCODER\")\n",
    "data = (torch.rand(size=(1, 3, 256, 256)))\n",
    "AE = AutoEncoder()\n",
    "img_out = AE(data)\n",
    "print(\"Latent's Shape:\", img_out[0].shape)\n",
    "print(\"Output's ShapeL\", img_out[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNET\n",
      "Output's Shape torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        # return self.softmax(self.conv(dec1))\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    \n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (name + \"conv1\",nn.Conv2d( in_channels=in_channels, out_channels=features, kernel_size=3, padding=1, bias=False)),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    ( name + \"conv2\", nn.Conv2d( in_channels=features, out_channels=features, kernel_size=3, padding=1, bias=False, )),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(\"\\nUNET\")\n",
    "data = img_out[1]\n",
    "unet = UNet()\n",
    "seg_out = unet(data)\n",
    "print(\"Output's Shape\", seg_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Image2Image and Reconstructed Image to Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model\n",
      "Latent's Shape:  torch.Size([4, 512, 4, 4])\n",
      "Reconstructed Image's Shape:  torch.Size([4, 3, 256, 256])\n",
      "Segmentation Mask's Shape:  torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class Image2Image2Mask(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Image2Image2Mask, self).__init__()\n",
    "\n",
    "        self.image2imageAE = AutoEncoder()\n",
    "        self.unet = UNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        imageLatent, reconsImage = self.image2imageAE(x)\n",
    "        segMask = self.unet(reconsImage)\n",
    "        return imageLatent, reconsImage, segMask\n",
    "\n",
    "\n",
    "print(\"Combined Model\")\n",
    "data = (torch.rand(size=(4, 3, 256, 256)))\n",
    "i2i2m = Image2Image2Mask()\n",
    "imageLatent, reconsImage, segMask = i2i2m(data)\n",
    "print(\"Latent's Shape: \", imageLatent.shape)\n",
    "print(\"Reconstructed Image's Shape: \", reconsImage.shape)\n",
    "print(\"Segmentation Mask's Shape: \", segMask.shape)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Model class to train, test, validate and infer the Image-to-Image Autoencoder Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    " \n",
    "    def __init__(self, trained=False):\n",
    "        self.model = AutoEncoder.to(DEVICE)\n",
    "\n",
    "\n",
    "    def train(self, dataset, loss_func, optimizer):\n",
    "\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        counter = 0\n",
    "        \n",
    "        for i, img in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            counter += 1\n",
    "            image = img.to(DEVICE)\n",
    "            outputs = self.model(image)\n",
    "            loss = loss_func(output, img)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate accuracy\n",
    "            pred = outputs.argmax(1)\n",
    "            correct = pred == label\n",
    "            running_correct += correct.sum().item()\n",
    "\n",
    "        # loss and accuracy for a complete epoch\n",
    "        epoch_loss = running_loss / (counter*BATCH_SIZE)\n",
    "        epoch_acc = 100. * (running_correct / (counter*BATCH_SIZE))\n",
    "\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self, dataset):\n",
    "\n",
    "        self.model.eval()\n",
    "        running_correct = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "                img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "                outputs = self.model(img)\n",
    "                #calculate accuracy\n",
    "                pred = outputs.argmax(1)\n",
    "                correct = pred == label\n",
    "                running_correct += correct.sum().item()\n",
    "                counter += 1\n",
    "\n",
    "        # loss and accuracy for a complete epoch\n",
    "        epoch_acc = 100. * (running_correct / (counter*BATCH_SIZE))\n",
    "        return epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, dataset):\n",
    "        running_correct = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        num = random.randint(0, len(dataset)//(BATCH_SIZE//2))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "                img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "                outputs = self.model(img)\n",
    "\n",
    "                #calculate accuracy\n",
    "                pred = outputs.argmax(1)\n",
    "                correct = pred == label\n",
    "                running_correct += correct.sum().item()\n",
    "                counter += 1\n",
    "                \n",
    "                if i == num:\n",
    "                    try:\n",
    "                        os.makedirs(f\"saved_samples/{MODEL_NAME}\", exist_ok=True)\n",
    "                    except:\n",
    "                        pass\n",
    "                    sample = random.randint(0, BATCH_SIZE//2)\n",
    "                    image = img[sample, :, :, :].cpu().numpy().transpose((1, 2, 0))\n",
    "                    image = (image * 255).astype('uint8')\n",
    "                    image = Image.fromarray(image)\n",
    "                    draw = ImageDraw.Draw(image)\n",
    "                    real_label = self.classes[label[sample].item()]\n",
    "                    pred_label = self.classes[pred[sample].item()]\n",
    "                    draw.text((image.width - 200, 0), f\"Real: {real_label}\", fill='red')\n",
    "                    draw.text((image.width - 200, 20), f\"Predicted: {pred_label}\", fill='blue')\n",
    "                    image.save(f\"saved_samples/{MODEL_NAME}/{num}.jpg\")\n",
    "\n",
    "        # loss and accuracy for a complete epoch\n",
    "        epoch_acc = 100. * (running_correct / (counter*BATCH_SIZE))\n",
    "    \n",
    "        return epoch_acc\n",
    "\n",
    "\n",
    " \n",
    "    def fit(self, epochs, lr):\n",
    "\n",
    "        print(f\"Using {DEVICE} device...\")\n",
    "        print(\"Loading Datasets...\")\n",
    "        train_data, val_data, test_data = get_dataloader(\"\", BATCH_SIZE)\n",
    "        print(\"Dataset Loaded.\")\n",
    "\n",
    "        print(\"Initializing Parameters...\")\n",
    "        self.model = self.model.to(DEVICE)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"The total parameters of the model are: {total_params}\")\n",
    "\n",
    "        print(f\"Initializing the Optimizer\")\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr)\n",
    "        print(f\"Beginning to train...\")\n",
    "\n",
    "        diceloss = nn.DiceLoss()\n",
    "\n",
    "        val_jacc_epochs = []\n",
    "        writer = SummaryWriter(f'runs/{MODEL_NAME}/')\n",
    "        os.makedirs(\"checkpoints/\", exist_ok=True)\n",
    "        os.makedirs(\"saved_model/\", exist_ok=True)\n",
    "\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            print(f\"Epoch No: {epoch}\")\n",
    "\n",
    "            train_loss, train_jacc = self.train(dataset=train_data, loss_func=diceloss, optimizer=optimizer)\n",
    "\n",
    "            val_jacc = self.validate(dataset=val_data)\n",
    "            val_jacc_epochs.append(val_jacc)\n",
    "\n",
    "            print(f\"Train Loss:{train_loss}, Train Jaccard Score:{train_jacc}, Validation Jaccard Score:{val_jacc}\")\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Jacc/Train\", train_jacc, epoch)\n",
    "            writer.add_scalar(\"Jacc/Val\", val_jacc, epoch)\n",
    "\n",
    "\n",
    "            if max(val_jacc_epochs) == val_jacc:\n",
    "                torch.save(self.model.state_dict(), f\"checkpoints/{MODEL_NAME}.pth\")\n",
    "            \n",
    "            if epoch%10==0:\n",
    "                print(\"Saving model\")\n",
    "                torch.save(self.model.state_dict(), f\"saved_model/{MODEL_NAME}_{epoch}.pth\")\n",
    "                test_jacc = self.test(dataset=test_data)\n",
    "                writer.add_scalar(\"Jacc/Test\", test_jacc)\n",
    "                print(\"Model Saved\")\n",
    "\n",
    "    \n",
    "            print(\"Epoch Completed. Proceeding to next epoch...\")\n",
    "\n",
    "        print(f\"Training Completed for {epochs} epochs.\")\n",
    "\n",
    "\n",
    "    def infer_a_random_sample(self):\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(f\"test_samples/{MODEL_NAME}\", exist_ok=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        with open('Dataset/Plant_Village/test.csv', newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            rows = list(csvreader)\n",
    "            random_row = random.choice(rows)\n",
    "            path = random_row[0]\n",
    "            label = random_row[1]\n",
    "\n",
    "            image = Image.open(path)\n",
    "            imageT = transform(image).unsqueeze(0).to(DEVICE)\n",
    "            outputs = self.model(imageT)\n",
    "            pred = outputs.argmax(1)\n",
    "            pred_label = self.classes[pred.item()]\n",
    "            print(pred_label)\n",
    "            print(label)\n",
    "\n",
    "\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            draw.text((image.width - 200, 0), f\"Real: {label}\", fill='red')\n",
    "            draw.text((image.width - 200, 20), f\"Predicted: {pred_label}\", fill='blue')\n",
    "            image.save(f\"test_samples/{MODEL_NAME}/{label} -> {pred_label}.jpg\")\n",
    "            print(\"Saved a sample\")\n",
    "\n",
    "\n",
    "\n",
    "    def infer_a_sample(self, image):\n",
    "        \n",
    "        # Forward pass the image through the model.\n",
    "        prediction = self.model(image)\n",
    "\n",
    "        # Get the class with the highest probability.\n",
    "        class_index = prediction.argmax(1)\n",
    "\n",
    "        # Get the class name.\n",
    "        class_name = self.classes[class_index.item()]\n",
    "        return class_name\n",
    "\n",
    "\n",
    "\n",
    "# model = Model()\n",
    "# model.fit(1000, 5e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
